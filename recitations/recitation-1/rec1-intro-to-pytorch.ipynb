{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recitation 1 - Intro to PyTorch\n",
    "\n",
    "Reference: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "\n",
    "TA: Xingjia Wang\n",
    "\n",
    "xw400@rutgers.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install PyTorch. Skip if you already have it installed.\n",
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries (import torch first to prevent kernel crashes)\n",
    "import torch\n",
    "import torch.optim as optim                                 # torch.optim is a module that includes various optimization algorithms.\n",
    "import torch.nn as nn                                       # torch.nn is a module for building neural network layers.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random seed setting\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch, and what can it do?\n",
    "\n",
    "PyTorch is the fastest growing Deep Learning framework that enables:\n",
    "- Tensor computation (like NumPy) with strong GPU acceleration\n",
    "- Deep neural networks built on a tape-based autograd system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Simple Linear Regression\n",
    "\n",
    "Let's see how PyTorch can help us solve a predictive problem using the most **simple linear regression model**. \n",
    "\n",
    "As you recall from your intro to statistics class, a **simple linear regression model** assumes that the observed data sample $y_1, y_2, ..., y_n$ follows the following structure:\n",
    "\n",
    "$$\n",
    "y_i = a + bx_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Let's start by creating some synthetic data to work with. The following code block does the following:\n",
    "\n",
    "1. We generate $x_1, x_2, ..., x_{100}$ with each $x_i \\sim \\text{Uniform}(0,1)$. \n",
    "2. We let $a = 1$ and $b = 2$.\n",
    "3. We create $y_1, y_2, ..., y_{100}$ by setting $y_i = 1 + 2x_i + \\epsilon_i$, where $\\epsilon_i$ is some standard Gaussian noise.\n",
    "\n",
    "**Let's run the following block.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "x = np.random.rand(100, 1)                              # np.random.rand generates random samples from a uniform distribution over [0, 1).\n",
    "                                                        # the resulting dimension of vector x will be 100 x 1.\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)            # np.random.randn generates random samples from a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation Split\n",
    "\n",
    "Now we do a train-validation split, by randomly picking 80% of the indices as the train set and the rest as validation.\n",
    "**Let's run the following block.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Below shows a plot of the two sets of data.\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "axes[0].scatter(x_train, y_train, c = 'darkblue')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Generated Data - Train')\n",
    "axes[1].scatter(x_val, y_val, c = 'darkred')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_title('Generated Data - Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to PyTorch\n",
    "\n",
    "We hope to fit a regression model to the above data through a neural network. We will be building this neural network in **PyTorch** in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors. ###\n",
    "x_train_tensor =   \n",
    "y_train_tensor = \n",
    "\n",
    "# note: .to(device) if GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation via Gradient Descent\n",
    "\n",
    "We know that the true $a = 1$ and $b = 2$, but now let’s see how close we can get to the true values by using **gradient descent** and the 80 points in the training set.\n",
    "\n",
    "Note: if we use all data points $\\{(y_i, x_i)\\}_{i=1}^N$ in the training set to compute the loss, we are performing a **batch gradient descent**. If we were to use a single, randomly picked point $(y_i, x_i)$ at each time, it would be a **stochastic gradient descent**. Anything else in-between characterizes a **mini-batch gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters and Require Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6390], requires_grad=True) tensor([-0.6285], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "\n",
    "### TODO: Initialize parameters... ###\n",
    "a = \n",
    "b = \n",
    "### ---------------------- ###\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computation with **Autograd**\n",
    "\n",
    "Autograd is PyTorch’s automatic differentiation package, which refrains us from manually computing all the gradients.\n",
    "\n",
    "- The backward() method helps us to compute partial derivatives of the loss function w.r.t. our parameters.\n",
    "- We obtain the computed gradients via the .grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2234])\n",
      "tensor([-1.0651])\n"
     ]
    }
   ],
   "source": [
    "# Specifying a learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "### TODO: implement the following... ###\n",
    "yhat = \n",
    "error = \n",
    "loss = \n",
    "### ---------------------- ###\n",
    "\n",
    "# We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "loss.backward()\n",
    "# Let's check the computed gradients...\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the parameters, we need to use torch.no_grad() to perform regular Python operations on tensors, independent of PyTorch’s computation graph. \n",
    "\n",
    "After updating, we need to clear the gradients calculated in this past step to avoid them from accumulating via the zero_() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: update the parameters ###\n",
    "### ---------------------- ###\n",
    "\n",
    "### TODO: PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go... ###\n",
    "### ---------------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8781])\n",
      "tensor([-0.8820])\n",
      "tensor([0.0383])\n",
      "tensor([-0.0750])\n",
      "tensor([0.0084])\n",
      "tensor([-0.0164])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0036])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([8.6969e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([1.9059e-05])\n",
      "tensor([-3.6987e-05])\n",
      "tensor([4.2216e-06])\n",
      "tensor([-8.0911e-06])\n",
      "tensor([9.6973e-07])\n",
      "tensor([-1.8024e-06])\n",
      "tensor([5.4949e-07])\n",
      "tensor([-5.7005e-07])\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "\n",
    "    ### TODO: We just tell PyTorch to work its way BACKWARDS from the specified loss! ###\n",
    "    ### ---------------------- ###\n",
    "\n",
    "    # Let's check the computed gradients...\n",
    "    if epoch % 100 == 0:\n",
    "        print(a.grad)\n",
    "        print(b.grad)\n",
    "    \n",
    "    ### TODO: Updating the parameters ###\n",
    "    ### ---------------------- ###\n",
    "\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update All Parameters Simultaneously with **Optimizer**\n",
    "\n",
    "Previously, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.\n",
    "\n",
    "An optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\n",
    "\n",
    "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!\n",
    "\n",
    "In the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializations: a = tensor([-0.8814], requires_grad=True), b = tensor([1.1271], requires_grad=True)\n",
      "Output: a = tensor([1.0235], requires_grad=True), b = tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(\"Initializations: a = {}, b = {}\".format(a, b))\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "\n",
    "### TODO: Defines a SGD optimizer to update the parameters ###\n",
    "optimizer = \n",
    "### ---------------------- ###\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "\n",
    "    ### TODO: perform the update via step() ###\n",
    "    ### ---------------------- ###\n",
    "\n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "\n",
    "    ### TODO: clearing the gradient ###\n",
    "    ### ---------------------- ###\n",
    "    \n",
    "print(\"Output: a = {}, b = {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Computation\n",
    "\n",
    "Once again, PyTorch get us covered for loss functions, and we don't have to manually write out the expressions. There are many loss functions to choose from, depending on the task at hand. Since ours is a regression, we are using the Mean Square Error (MSE) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializations: a = tensor([-0.6824], requires_grad=True), b = tensor([-0.5733], requires_grad=True)\n",
      "Output: a = tensor([1.0235], requires_grad=True), b = tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(\"Initializations: a = {}, b = {}\".format(a, b))\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "### TODO: Defines a MSE loss function ###\n",
    "loss_fn = \n",
    "### ---------------------- ###\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "\n",
    "    ### TODO: Incorporate loss function ###\n",
    "    loss = \n",
    "    ### ---------------------- ###\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(\"Output: a = {}, b = {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "In PyTorch, a model is represented by a regular Python class that inherits from the Module class.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "- \\_\\_init\\_\\_(self): it defines the parts that make up the model —in our case, two parameters, $a$ and $b$.\n",
    "- forward(self, x): it performs the actual computation, that is, it outputs a prediction, given the input $x$.\n",
    "\n",
    "Let’s build a proper (yet simple) model for our regression task. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define the following class ###\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "### ---------------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By wrapping our parameters as nn.Parameter objects, we can use our model’s parameters() method to retrieve an iterator over all model’s parameters, even those parameters of nested models, that we can use to feed our optimizer (instead of building a list of parameters ourselves!).\n",
    "\n",
    "Moreover, we can get the current values for all parameters using our model’s state_dict() method.\n",
    "\n",
    "Now, putting these methods all together, we can re-write our code as the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([-0.2573])), ('b', tensor([1.4762]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression()\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # In PyTorch, models have a train() method which, somewhat disappointingly, \n",
    "    # does NOT perform a training step. Its only purpose is to set the model to training mode.\n",
    "    \n",
    "    ### TODO: enter training mode ###\n",
    "    ### ---------------------- ###\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "\n",
    "    ### TODO: auto prediction ###\n",
    "    yhat =\n",
    "    ### ---------------------- ###\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making the Training Step More Generic (Optional)\n",
    "\n",
    "So far, we’ve defined an **optimizer**, a **loss function** and a **model**. We now want to generalize these processes as a **training step**, so that the code could be more organized when training on batches.\n",
    "\n",
    "We write a function that takes those three elements and returns another function that performs a training step, taking a set of features and labels as arguments and returning the corresponding loss. Then, we can use this general-purpose function to build a train_step() function to be called inside our training loop. \n",
    "\n",
    "Now our code should look like this… see how tiny the training loop is now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: implement the following function to generalize training step. ###\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        \n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "### ---------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing Package-Defined Layers\n",
    "\n",
    "For simple tasks like building a linear regression, we could directly utilize the basic building blocks defined in the package's torch.nn module. \n",
    "\n",
    "In our previous model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s Linear model as an attribute of our own, thus creating a nested model. \n",
    "\n",
    "To do so, we could simply just replace the parameters with nn.Linear(input, output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### TODO: Instead of our custom parameters, we use a Linear layer with single input and single output ###\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        ### ---------------------- ###\n",
    "                \n",
    "    def forward(self, x):\n",
    "        ### TODO: Now it only takes a call to the layer to make predictions ###\n",
    "        return \n",
    "        ### ---------------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we call the parameters() method of this model, PyTorch will figure the parameters of its attributes in a recursive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight tensor([[-0.0493]])\n",
      "linear.bias tensor([-0.9734])\n"
     ]
    }
   ],
   "source": [
    "for name, param in NestedLinearRegression().named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we don't even want to create a class for this simple task, we could also just use nn.Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqmodel =  ### TODO: define with nn.Sequential ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[0.9826]])\n",
      "0.bias tensor([0.2325])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seqmodel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models With Hidden Layers\n",
    "\n",
    "![One Layer Neural Network](one-layer-nn.png)\n",
    "\n",
    "Remember the XOR-problem in lecture 2? Now, does the code in the class notes make more sense to you?\n",
    "\n",
    "The below builds a feedforward neural network with 1 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hidden layer neural net\n",
    "class HiddenLayerLinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### TODO: Write a sequential model with one hidden layer ###\n",
    "        self.sequential = \n",
    "        ### ---------------------- ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sequential(x)\n",
    "        return y\n",
    "        \n",
    "    # def loss_fn(self, y, y_pred):\n",
    "    #     loss = y * torch.log(y_pred + 1e-8) + (1-y) * torch.log(1-y_pred + 1e-8)\n",
    "    #     output = -loss.sum()\n",
    "    #     return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above, we have a 2-dimensional input, a 2-dimensional hidden layer and a 1-dimensional of output. How many parameters would we end up having?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential.0.weight tensor([[-0.1374, -0.0386],\n",
      "        [-0.0058, -0.0886]])\n",
      "sequential.0.bias tensor([-0.1542,  0.3182])\n",
      "sequential.2.weight tensor([[-0.0983,  0.4479]])\n",
      "sequential.2.bias tensor([0.1999])\n"
     ]
    }
   ],
   "source": [
    "for name, param in HiddenLayerLinearRegression(2,2).named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data in Datasets\n",
    "\n",
    "Until now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small data, sure, but for much larger data, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our data accordingly. \n",
    "\n",
    "To do so, we first introduce the Dataset class, where we can store our features and labels. We can either use a preset TensorDataset class, or build our own customized Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713], dtype=torch.float64), tensor([2.4745], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "### TODO: wrap our data in the preset TensorDataset class ###\n",
    "train_data = \n",
    "### ---------------------- ###\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader \n",
    "\n",
    "Now we introduce how we build mini-batches for batch gradient descent with PyTorch’s DataLoader class. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\n",
    "\n",
    "Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "### TODO: Load our dataset into Dataloader for batch calculations ###\n",
    "train_loader = \n",
    "### ---------------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve a sample mini-batch, the following will return a list containing two tensors: one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.9507],\n",
       "         [0.7852],\n",
       "         [0.1409],\n",
       "         [0.1560],\n",
       "         [0.8324],\n",
       "         [0.6842],\n",
       "         [0.5248],\n",
       "         [0.9869],\n",
       "         [0.7722],\n",
       "         [0.6075],\n",
       "         [0.5979],\n",
       "         [0.1220],\n",
       "         [0.9696],\n",
       "         [0.1960],\n",
       "         [0.6376],\n",
       "         [0.3585]], dtype=torch.float64),\n",
       " tensor([[2.8715],\n",
       "         [2.5283],\n",
       "         [1.1211],\n",
       "         [1.2901],\n",
       "         [2.6119],\n",
       "         [2.3492],\n",
       "         [2.0167],\n",
       "         [3.0520],\n",
       "         [2.4208],\n",
       "         [2.4037],\n",
       "         [2.0407],\n",
       "         [1.2406],\n",
       "         [2.8401],\n",
       "         [1.4393],\n",
       "         [2.1930],\n",
       "         [1.7462]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into our training loop yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0260])), ('b', tensor([1.9699]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    ### TODO: Iterate train step and append loss over batches ###\n",
    "    ### ---------------------- ### \n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier Train-Validation Split via random_split()\n",
    "\n",
    "The Dataset class also allows us to do the train-validation split in a quicker way. To do so, we just need to call random_split() on the whole Dataset object and specify the proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(x)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "### TODO: train-val split with random_split on the whole dataset ###\n",
    "train_dataset, val_dataset = \n",
    "\n",
    "train_loader = \n",
    "val_loader = \n",
    "### ---------------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "This is the last part of our journey — we need to change the training loop to include the evaluation of our model, that is, computing the validation loss. \n",
    "\n",
    "- Step 1: Include another inner loop to handle the mini-batches that come from the validation loader, sending them to the same device as our model. \n",
    "- Step 2: Make predictions using our model and compute the corresponding loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0328])), ('b', tensor([1.9385]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []                                                 # This is new!\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # send to device if needed\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "\n",
    "    ### TODO: perform validation ###    \n",
    "    ### ---------------------- ###\n",
    "    \n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two small important things to note:\n",
    "\n",
    "- torch.no_grad(): even though it won’t make a difference in our simple model, it is a good practice to wrap the validation inner loop with this context manager to disable any gradient calculation that you may inadvertently trigger — gradients belong in training, not in validation steps;\n",
    "- eval(): the only thing it does is setting the model to evaluation mode (just like its train() counterpart did), so the model can adjust its behavior regarding some operations, like Dropout.\n",
    "\n",
    "Further optimizations: using a learning rate scheduler, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
